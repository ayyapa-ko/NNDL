{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Step 1: Import Libraries ===\n",
    "from google.colab import drive\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# === Step 2: Extract Data ===\n",
    "zip_file = \"/content/train.zip\"\n",
    "extract_folder = \"/content/trainimages\"\n",
    "csv_file = \"/content/trainLabels.csv\"\n",
    "\n",
    "if not os.path.exists(extract_folder):\n",
    "    os.makedirs(extract_folder)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file, 'r') as archive:\n",
    "        archive.extractall(extract_folder)\n",
    "except zipfile.BadZipFile:\n",
    "    shutil.unpack_archive(zip_file, extract_folder)\n",
    "\n",
    "image_folder = \"/content/trainimages/train\"\n",
    "\n",
    "# === Step 3: Load Dataset ===\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "def load_image(image_id, img_size=(28, 28)):\n",
    "    img_path = os.path.join(image_folder, f\"{image_id}.png\")\n",
    "    if os.path.exists(img_path):\n",
    "        img = imread(img_path)\n",
    "        img_resized = resize(img, img_size)\n",
    "        img_gray = rgb2gray(img_resized)\n",
    "        return img_gray\n",
    "    return None\n",
    "\n",
    "# === Step 4: Preprocess Data ===\n",
    "X_data, y_data = [], []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    img = load_image(row[\"id\"])\n",
    "    if img is not None:\n",
    "        X_data.append(img)\n",
    "        y_data.append(row[\"label\"])\n",
    "\n",
    "X_data = np.array(X_data).reshape(len(X_data), 28, 28, 1)\n",
    "y_data = np.array(pd.factorize(y_data)[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# === Step 5: Design CNN Model (A1) ===\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), input_shape=(28, 28, 1), activation='relu', name=\"conv1\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu', name=\"conv2\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# === Step 6: Train CNN Model (A2) ===\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# === Step 7: Plot Training & Validation Loss (A3) ===\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "# === Step 8: Evaluate Model Accuracy (A4) ===\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('\\nCNN Model Accuracy:', score[1])\n",
    "\n",
    "# === Step 9: Feature Map Visualization (A1) ===\n",
    "sample_image = X_test[0].reshape(1, 28, 28, 1)\n",
    "\n",
    "model.predict(sample_image)\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]\n",
    "feature_map_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "feature_maps = feature_map_model.predict(sample_image)\n",
    "\n",
    "for layer_idx, feature_map in enumerate(feature_maps):\n",
    "    num_filters = feature_map.shape[-1]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.suptitle(f\"Feature Maps - Layer {layer_idx+1}\", fontsize=14)\n",
    "\n",
    "    for i in range(min(num_filters, 6)):\n",
    "        plt.subplot(1, 6, i+1)\n",
    "        plt.imshow(feature_map[0, :, :, i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Filter {i+1}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# === Step 10: Design & Train Fully Connected Model (A7) ===\n",
    "fc_model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "fc_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "fc_history = fc_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# === Step 11: Plot Training vs Validation Loss (A8) ===\n",
    "plt.plot(fc_history.history['loss'], label='Training Loss')\n",
    "plt.plot(fc_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Fully Connected Network - Training vs Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "# === Step 12: Test & Evaluate Fully Connected Model (A8) ===\n",
    "fc_score = fc_model.evaluate(X_test, y_test, verbose=1)\n",
    "print('\\nFully Connected Network Accuracy:', fc_score[1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
